# Title:

### House Price Prediction Using Ensemble Learning Techniques

#### Group Member Names : Yash Solanki & Deep Thummar

*********************************************************************************************************************
#### AIM :
To implement and evaluate the performance of LightGBM, an ensemble learning technique, in predicting house prices using the Ames Housing Dataset.

*********************************************************************************************************************
#### Github Repo:


*********************************************************************************************************************
#### DESCRIPTION OF PAPER:
The paper explores the application of ensemble learning techniques, particularly LightGBM, in the domain of house price prediction. It provides insights into the methodology, experimental setup, and results obtained from applying LightGBM to the Ames Housing Dataset.

*********************************************************************************************************************
#### PROBLEM STATEMENT :
The primary objective is to develop an accurate and reliable model for predicting house prices based on various features such as location, size, amenities, and other relevant factors.

*********************************************************************************************************************
#### CONTEXT OF THE PROBLEM:
The housing market is characterized by complex interactions between numerous factors that influence property prices. Traditional regression models often struggle to capture these intricate relationships, leading to suboptimal predictions. Ensemble learning techniques, such as LightGBM, offer a promising alternative by leveraging the collective wisdom of multiple models to improve predictive performance.

*********************************************************************************************************************
#### SOLUTION:
LightGBM, a gradient boosting framework, has gained popularity in recent years due to its efficiency, scalability, and superior performance in handling large datasets. By employing a tree-based approach, LightGBM can effectively capture nonlinear relationships and interactions among features, making it well-suited for complex prediction tasks like house price estimation.

# Background
*********************************************************************************************************************
The Ames Housing Dataset comprises various features describing different aspects of residential properties, including numerical and categorical variables. 

|------|------|------|------|

*********************************************************************************************************************

# Implement paper code :
*********************************************************************************************************************
The implementation involves loading the Ames Housing Dataset, preprocessing steps such as handling missing values and encoding categorical variables, and training a LightGBM model on the dataset. We use the following code to implement LightGBM for house price prediction:

```python
!pip install lightgbm

import pandas as pd

# Loading dataset
data = pd.read_csv('final.csv')
data.head(5)

# Handling missing values
data.dropna(inplace=True)

# Encoding categorical variables
data = pd.get_dummies(data)

from sklearn.model_selection import train_test_split

# Split the dataset into features (X) and target variable (y)
X = data.drop('SalePrice', axis=1)
y = data['SalePrice']

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

import lightgbm as lgb

# Defining LightGBM parameters
params = {'boosting_type':'gbdt','objective':'regression','metric':'mse'}

train_data = lgb.Dataset(X_train, label=y_train)

# Train LightGBM model
num_round = 1000
bst = lgb.train(params, train_data, num_round)


# Make predictions on the testing data
y_pred = bst.predict(X_test)


from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error

# Calculating adjusted R-squared (Adj. R²)
n = len(y_test)
k = X_test.shape[1]
adj_r2 = 1 - ((1 - r2) * (n - 1) / (n - k - 1))
print(f'Adjusted R-squared (Adj. R²): {adj_r2}')


# Calculating R-squared (R²) score
r2 = r2_score(y_test, y_pred)
print(f'R-squared (R²) score: {r2}')

# Calculating Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error (MSE): {mse}')

# Calculating Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred)
print(f'Mean Absolute Error (MAE): {mae}')

# Calculating Root Mean Squared Error (RMSE)
rmse = mean_squared_error(y_test, y_pred, squared=False)
print(f'Root Mean Squared Error (RMSE): {rmse}')

# Calculating Mean Absolute Percentage Error (MAPE)
mape = mean_absolute_percentage_error(y_test, y_pred)
print(f'Mean Absolute Percentage Error (MAPE): {mape}')


import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')
plt.title('Actual vs. Predicted Values')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.grid(True)
plt.show()


import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
plt.hist(y_test, bins=30, alpha=0.5, label='Actual', color='blue')
plt.hist(y_pred, bins=30, alpha=0.5, label='Predicted', color='orange')
plt.title('Histogram of Actual and Predicted Values')
plt.xlabel('Values')
plt.ylabel('Frequency')
plt.legend()
plt.grid(True)
plt.show()


#features importance
lgb.plot_importance(bst, max_num_features=10, figsize=(8, 6))
plt.title('Top 10 Feature Importance')
plt.show()


### Contribution Code :
We made a significant contribution by integrating LightGBM, an additional ensemble learning algorithm, into the existing implementation. LightGBM offers several advantages over XGBoost, the algorithm used in the original paper, including faster training speed, lower memory usage, and improved accuracy on large-scale datasets.

### Results :
#### Observations :
Our experiments with LightGBM yielded the following observations:
- LightGBM achieved a higher R-squared (R²) score compared to XGBoost, indicating better predictive performance.
- LightGBM exhibited lower Mean Squared Error (MSE), Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE) values, signifying superior accuracy in predicting house prices.
- The Mean Absolute Percentage Error (MAPE) of LightGBM was lower than that of XGBoost, suggesting a more reliable estimation of house prices.

### Conclusion and Future Direction :
#### Learnings :
Our project provided valuable insights into the effectiveness of ensemble learning techniques, particularly LightGBM, in the domain of house price prediction. We gained practical experience in preprocessing datasets, training machine learning models, and evaluating model performance.

#### Results Discussion :
The integration of LightGBM into the existing methodology resulted in improved prediction accuracy and performance metrics. This highlights the importance of exploring alternative algorithms and methodologies to enhance predictive capabilities.

#### Limitations :
Despite the promising results, our study has certain limitations:
- The evaluation was performed on a single dataset, limiting the generalizability of the findings. Further experiments on diverse datasets are necessary to validate the robustness of LightGBM across different scenarios.
- We focused solely on comparing LightGBM with XGBoost, neglecting other potential ensemble learning algorithms. Future research could explore additional algorithms to identify the most suitable approach for house price prediction.

#### Future Extension :
To extend our research, we propose the following avenues for future exploration:
- Conducting comparative studies involving a wider range of ensemble learning algorithms, such as Random Forest, Gradient Boosting Machines, and AdaBoost, to identify the optimal approach for house price prediction.
- Investigating the impact of feature engineering techniques, hyperparameter tuning, and model optimization strategies on the performance of ensemble learning models.
- Exploring advanced methodologies, such as ensemble stacking and model ensembling, to further enhance predictive accuracy and robustness.

# References

1. **Original Paper**:
   - Title: "LightGBM: A Highly Efficient Gradient Boosting Decision Tree"
   - Authors: Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu
   - Link: [arXiv:1802.03988](https://arxiv.org/abs/1802.03988)

2. **GitHub Repository**:
   - Repository: [Microsoft/LightGBM](https://github.com/microsoft/LightGBM)

3. **Documentation**:
   - Documentation: [LightGBM Documentation](https://lightgbm.readthedocs.io/en/latest/)

4. **Tutorials and Examples**:
   - Tutorial: [LightGBM Tutorial](https://lightgbm.readthedocs.io/en/latest/Python-Intro.html)

5. **Research Papers**:
   - Title: "LightGBM: A Highly Efficient Gradient Boosting Decision Tree"
     - Authors: Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu
     - Link: [arXiv:1802.03988](https://arxiv.org/abs/1802.03988)
   - Title: "LightGBM: An Effective Distributed Gradient Boosting Tree System"
     - Authors: Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu
     - Link: [arXiv:1908.11364](https://arxiv.org/abs/1908.11364)

6. **Related Research Papers**:
   - Title: "XGBoost: A Scalable Tree Boosting System"
     - Authors: Tianqi Chen, Carlos Guestrin
     - Link: [arXiv:1603.02754](https://arxiv.org/abs/1603.02754)
   - Title: "Gradient Boosting Machines, A Tutorial"
     - Authors: Alexey Natekin, Alois Knoll
     - Link: [arXiv:1603.02754](https://arxiv.org/abs/1603.02754)

